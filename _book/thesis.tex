\documentclass{aucklandthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add any LaTeX packages and other preamble here if required
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{ZHAOMING SU}
\title{Data Storytelling Dashboard for Exploring Auckland Air Quality}
\degrees{Bachelor of Science (Honours) in Statistics}
\supervisor{Dr.~Earo Wang}
\cosupervisor{Prof.~Chris Wild}
\def\degreetitle{}
% Add subject and keywords below
\hypersetup{
     %pdfsubject={The Subject},
     %pdfkeywords={Some Keywords},
     pdfauthor={ZHAOMING SU},
     pdftitle={Data Storytelling Dashboard for Exploring Auckland Air Quality},
     pdfproducer={Bookdown with LaTeX}
}


\bibliography{thesisrefs}

\begin{document}

\pagenumbering{roman}

\titlepage

{\setstretch{1.2}\rm\tighttoc\doublespacing}

\hypertarget{abstract}{%
\chapter*{Abstract}\label{abstract}}
\addcontentsline{toc}{chapter}{Abstract}

The abstract should outline the main approach and findings of the thesis and must not be more than 500 words.

\newpage

\hypertarget{acknowledgements}{%
\chapter*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{Acknowledgements}

I would like to thank my pet goldfish for \dots

\hypertarget{copyright-notice}{%
\chapter*{Copyright notice}\label{copyright-notice}}
\addcontentsline{toc}{chapter}{Copyright notice}

\textcopyright { } \authorname~(\number\the\year).

I certify that I have made all reasonable efforts to secure copyright permissions for third-party content included in this thesis and have not knowingly added copyright content to my work without the owner's permission.

\newpage

\hypertarget{declaration}{%
\chapter*{Declaration}\label{declaration}}
\addcontentsline{toc}{chapter}{Declaration}

This dissertation is an original work of my research and contains no material which has been accepted for the award of any other degree or diploma at any university or equivalent institution and that, to the best of my knowledge and belief, this dissertation contains no material previously published or written by another person, except where due reference is made in the text of the dissertation.

\hypertarget{ch:intro}{%
\chapter{Introduction (Template Demo)}\label{ch:intro}}

This is where you introduce the main ideas of your thesis, and an overview of the context and background.

In a PhD, Chapter 2 would normally contain a literature review. Typically, Chapters 3--5 would contain your own contributions. Think of each of these as potential papers to be submitted to journals. Finally, Chapter 6 provides some concluding remarks, discussion, ideas for future research, and so on. Appendixes can contain additional material that don't fit into any chapters, but that you want to put on record. For example, additional tables, output, etc.

\hypertarget{rmarkdown}{%
\section{Rmarkdown}\label{rmarkdown}}

In this template, the rest of the chapter shows how to use Rmarkdown. The big advantage of using Rmarkdown is that it allows you to include your R code directly into your thesis, to ensure there are no errors in copying and pasting, and that everything is reproducible. It also helps you stay better organized.

For details on using \emph{R Markdown} see \url{http://rmarkdown.rstudio.com}.

\hypertarget{data}{%
\section{Data}\label{data}}

Included in this template is a file called \texttt{sales.csv}. This contains quarterly data on Sales and Advertising budget for a small company over the period 1981--2005. It also contains the GDP (gross domestic product) over the same period. All series have been adjusted for inflation. We can load in this data set using the following command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sales <-}\StringTok{ }\KeywordTok{ts}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/sales.csv"}\NormalTok{)[, }\DecValTok{-1}\NormalTok{], }\DataTypeTok{start =} \DecValTok{1981}\NormalTok{, }\DataTypeTok{frequency =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Any data you use in your thesis can go into the data directory. The data should be in exactly the format you obtained it. Do no editing or manipulation of the data outside of R. Any data munging should be scripted in R and form part of your thesis files (possibly hidden in the output).

\hypertarget{figures}{%
\section{Figures}\label{figures}}

Figure \ref{fig:deaths} shows time plots of the data we just loaded. Notice how figure captions and references work. Chunk names can be used as figure labels with \texttt{fig:} prefixed. Never manually type figure numbers, as they can change when you add or delete figures. This way, the figure numbering is always correct.

\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/deaths-1.pdf}
\caption{\label{fig:deaths}Quarterly sales, advertising and GDP data.}
\end{figure}

\hypertarget{results-from-analyses}{%
\section{Results from analyses}\label{results-from-analyses}}

We can fit a dynamic regression model to the sales data.

If \(y_t\) denotes the sales in quarter \(t\), \(x_t\) denotes the corresponding advertising budget and \(z_t\) denotes the GDP, then the resulting model is:
\begin{equation}
  y_t - y_{t-4} = \beta (x_t-x_{t-4}) + \gamma (z_t-z_{t-4}) + \theta_1 \varepsilon_{t-1} + \Theta_1 \varepsilon_{t-4} + \varepsilon_t
\end{equation}
where
\(\beta = 2.28\),
\(\gamma = 0.97\),
\(\theta_1 = NA\),
and
\(\Theta_1 = -0.90\).

\hypertarget{tables}{%
\section{Tables}\label{tables}}

Let's assume future advertising spend and GDP are at the current levels. Then forecasts for the next year are given in Table \ref{tab:salesforecasts}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lllll}
\toprule
Point Forecast & Lo 80 & Hi 80 & Lo 95 & Hi 95 \\
\midrule
1000.2 &  947.7 & 1052.7 & 919.9 & 1080.5 \\
1013.1 &  959.3 & 1066.8 & 930.9 & 1095.3 \\
1076.7 & 1022.9 & 1130.6 & 994.4 & 1159.0 \\
1003.5 &  949.7 & 1057.4 & 921.2 & 1085.8 \\
\bottomrule
\end{tabular}
\caption{Forecasts for the next year assuming Advertising budget and GDP are unchanged.}
\label{tab:salesforecasts}
\end{center}
\end{table}

Again, notice the use of labels and references to automatically generate Table numbers. In this case, we need to generate the label ourselves.

The \texttt{knitLatex} package is useful for generating tables from R output. Other packages can do similar things including the \texttt{kable} function in \texttt{knitr} which is somewhat simpler but you have less control over the result. If you use \texttt{knitLatex} to generate tables, don't forget to include \texttt{results="asis"} in the chunk settings.

\hypertarget{ch:litreview}{%
\chapter{Background and related works}\label{ch:litreview}}

\hypertarget{tidy-time-series-data-wrangling-toolbox}{%
\section{Tidy time series data-wrangling toolbox}\label{tidy-time-series-data-wrangling-toolbox}}

The \textbf{tsibble} package offers a data infrastructure for wrangling time series data \autocite{tsibble}. A time series data set consists of one or more sequences indexed by time, often with a regular interval. As such, data-wrangling processes of time series data need to account for the special requirements of time series data analysis, including the explicit identification of time gaps and a method of handling multiple time series in a single data set for identifying duplicate records.

Analyses of fixed-interval time series require the data to be free from missing value, especially when the series is self-dependent. Whilst the explicit missing values can be easily handled by the substitution with interpolated values, the implicit gaps with missing index values are often neglected. In the case of multiple time series, locations of implicit time gaps may be different in each sequence; filling the gaps with traditional loops can be time-consuming and inefficient. \textbf{tsibble} identifies implicit time gaps with the \emph{index} and \emph{key} variables, such that each variable in the tsibble object is uniquely identified by the index and the interaction of all keys. As such, each time series is uniquely identified by the keys, allowing efficient identification of implicit time gaps, which is achieved by \textbf{tsibble} with a range of wrangling verbs.

Duplicates exist in different forms in cross-sectional and time series data. Typically, duplicates are identical observations exhibited as rows in a data frame, yet such definition is inadequate in identifying duplicates in time series data. There exists only one true value at any given point in time for each time series, meaning that there may be duplicate values that are non-identical observations with identical key-index pairs yet different in values. Instead of searching merely for duplicate rows, \textbf{tsibble} checks for duplicate key-index pairs. To avoid negligence, the creation of tsibble will fail upon detected duplicates.

\hypertarget{time-series-graphics-toolbox}{%
\section{Time series graphics toolbox}\label{time-series-graphics-toolbox}}

\hypertarget{calendar-graphics}{%
\subsection{Calendar graphics}\label{calendar-graphics}}

Calendars are the systematic partition of time from the observed solar-lunar phenomena and cultural custom, which is usable as graphics for temporal representations of societal activities and natural events. Calendar graphics are the method for the aggregated visualisation of time series data at sub-daily intervals, depicting the temporal dimension of time series data as the spatial layout in the calendar grid. The motivation of utilising calendars for data visualisations arises from the convenience of displaying observations in association with exact dates.

Air quality data are conventionally collected at hourly intervals, from which a time series plot becomes overcrowded, impeding the visual detections of abnormalities. \textcite{calmap} depicted a method of non-cartesian heatmaps on a calendar coordinate for visualising air pollution data. Prior to the paper, \textcite{calmapi} used calendar heatmaps to analyse the correlation of particulate matter temporally. Calendar graphics highlight the abnormalities and allow the association of the detected abnormalities to events with dates, providing insights and directions for analyses.

Calendar graphics is an application of trellis displays \autocite{trellis}, which spatially modularise the temporal dimension into conditional groups of small multiples \autocite{tufte} using calendar period (i.e., month and year) as an integrated and aligned plot. The expanded layout of the temporal dimension on a plane eases the cognitive load \autocite{tufte} in temporally locating the date of the events and extracting the date components, contrary to conventional time series plots.

\begin{figure}
\includegraphics[width=1\linewidth]{figures/cal-demo} \caption{A demonstration of a basic calendar heatmap with \textbf{Echarts.js} \autocite{echarts} faceted (unaligned) by \texttt{year\ \textasciitilde{}\ month}. Each tile corresponds to the value of an observed day or aggregated sub-daily observations, whose exact date and day of the week are easily identifiable.}\label{fig:cal-demo}
\end{figure}



\hypertarget{time-series-plots}{%
\subsection{Time series plots}\label{time-series-plots}}

In most scenarios, visualising time series relies on connecting points of observations with lines, curves or splines \autocite{dataviz-ts}, such that the sequences are plotted against the time index by positions along common xy-scales on the Cartesian coordinate \autocite{fpp3}. Connected time plots are the most elementary visual method for spotting extrinsic features in time series, including trends, seasons, cycles, clustering and oscillations.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/ctp-demo-1} \caption{A demonstration of a basic line plot with the \textbf{ggplot2} package \autocite{ggplot2} for the weekly economy passenger load on Ansett Airlines \autocite{fpp3d}. Visual analysis shows weak trend and cycle and abnormal zero values to be investigated. The presence of clustering also indicates a positive temporal dependence in the time series. It is nonetheless uneasy to align any observation to a date accurately.}\label{fig:ctp-demo}
\end{figure}



Based on connected time plots, \textcite{feasts} proposed the seasonal plot as a method of visualising seasonal patterns in the \textbf{feasts} package. The method conditionally subsets the complete time series into partitions of seasonal periods, each to be plotted in a homogeneous time plot and distinguished using a gradient colour scale for longitudinal comparison between periods.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/stp-demo-1} \caption{A \textbf{ggplot2}-based seasonal time series plot with the \textbf{feasts} package \autocite{feasts} for the monthly anti-diabetic drug sales in Australia \autocite{fpp3d}. The plot allows for both visualisation of intra-seasonal patterns and inter-seasonal variations shown as a positive trend in drug sales from year to year.}\label{fig:stp-demo}
\end{figure}



Nevertheless, interpreting connected time plots relies on the visual alignment of positions to the xy-scales in the Cartesian coordinates. Such visual alignments can be challenging upon the absence of explicit gridlines and axes, such as when the plot is fitted as a part of trellis displays \autocite{trellis}, in which the trend and scale of variations become ambiguous. As such, it is common to fill the area under the curve to emphasise the temporal variation in the plot \autocite{dataviz-ts}.

\hypertarget{html-widgets-for-interactive-graphics}{%
\section{HTML widgets for interactive graphics}\label{html-widgets-for-interactive-graphics}}

\hypertarget{developing-web-applications-with-r}{%
\subsection{Developing web applications with R}\label{developing-web-applications-with-r}}

The \textbf{shiny} package \autocite{shiny} provides a framework for developing web applications with \textbf{R} code \autocite{R2021}, both user and developer-friendly. It enables \textbf{R} users with no prior knowledge of HTML, CSS and JavaScript to create custom web applications with sophisticated functionality with template UI components and a server powered with reactive programming.

Reactive programming is the core of computation logic behind \textbf{shiny} \autocite{mshiny}, greatly simplifies the design of workflow, focusing only on evaluating the changes of values over time. Each change in reactive values is observed as an event by pre-defined callback functions, and workflows are executed as responses to events observed. The lazy nature of reactivity avoids repeated evaluations of expressions leading to wastage in computational resources. Reactive programming also allows users to define abstract workflows without conceiving the low-level data and programming logic by restricting evaluations to merely reactions to events, including user actions and internal value changes. A single reactive value can be observed and called by several callback functions, which can be shared across different functionalities. As the reactive value always keeps the previous evaluated result, conflicts among functionalities are avoided.

The user interface of \textbf{shiny} applications provides the front-end inputs and output display from the back end server logic. The collection of user input is the primary source of change of reactive values, events that trigger the evaluation of expressions in the back end server logic. The results of the evaluated expressions are rendered as the outputs, which may be as simple as prints of R objects or as sophisticated as HTML interactive graphics.

\hypertarget{sec:int-graphics}{%
\subsection{Interactive graphics with Echarts JavaScript in R}\label{sec:int-graphics}}

The \textbf{echarts4r} package \autocite{echarts4r}, the implementation of Echarts JavaScript in \textbf{R} \autocite{echarts}, offers access to a powerful open-source JavaScript library for interactive graphics. The creation of Echarts graphics with \textbf{echarts4r} adapts the pipe workflow with \textbf{dplyr} \texttt{\%\textgreater{}\%} \autocite{dplyr,magrittr}. The wrapper functions of \textbf{echarts4r} enable the configuration of graphic options and parameters without the need of understanding JavaScript syntax.

The creation of echarts4r objects follows the layer-to-layer approach, by which each function either (re-)initiates a chart, adds a single layer of graphical primitive or configures the options. It follows that the number of \textbf{echarts4r} function calls needed will be at least the number of graphic primitives to be mapped. In the case of mapping multiple time series as in seasonal plots (Section ref(sec:ts-plot)) with \textbf{echarts4r}, loops may be needed.

The underlying data passed to Echarts is treated slightly differently from the tidy data \autocite{tidy} underpinning \textbf{ggplot2}: each row represents an observation, and each column represents a variable. \textbf{echarts4r} converts the data frame into a JSON object upon initialisation, with each variable treated as a \emph{serie}. Such a difference implies a different logic for mapping the variables to graphical primitives, especially when mapping unordered categorical data.

It is worth noting that the mapping logic of Echarts in some circumstances is inconsistent with the conventional data organisation of tsibble upon plotting multivariate time series. The latter uniquely maps each observation to a single data point on the plot in a one-to-one relationship between the variables and graphical primitives, yet Echarts maps each cell value to a separate graphical primitive such that each time series is plotted with an x-y pair of two \emph{serie} instead of grouping the observations with tsibble keys. Thus, it is often necessary to ``widen'' the data frame, breaking the time series into separate columns of variables named by the tsibble keys, achievable with the \textbf{tidyr} package \autocite{tidyr}.

\hypertarget{sec:drill-down}{%
\subsection{Drill-down in interactive graphics}\label{sec:drill-down}}

\hypertarget{interactive-data-tables}{%
\subsection{Interactive data tables}\label{interactive-data-tables}}

\hypertarget{interactive-maps}{%
\subsection{Interactive maps}\label{interactive-maps}}

\hypertarget{visual-analysis-for-air-quality-data}{%
\section{Visual analysis for air quality data}\label{visual-analysis-for-air-quality-data}}

\begin{itemize}
\tightlist
\item
  Wind rose and pollution rose
\item
  Trend estimate fit plot and ACF
\end{itemize}

\hypertarget{ch:data}{%
\chapter{Auckland air quality data}\label{ch:data}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Air quality index (AQI) is a critical indicator of overall air quality by measuring key air pollutant concentrations at a given time. The constitution of AQI consists of ambient air pollutants listed in the National Environmental Standards for Ambient Air Quality which defines the threshold target for calculating AQI \autocite{aqi}. The national standard defines AQI as the maximum ambient air pollutant measurement ratio to the national target as a percentage \autocite{aqitarget}. Over 10 stations in Auckland monitor a subset of the standard-listed pollutants in an hourly interval.

It is of interest to explore the variation and relationships of AQI and its constituent pollutants with other environmental and meteorological parameters over time. The data, provided by \textcite{aklenvdata}, includes 14 parameters from 10 monitoring stations in Auckland from as North as Takapuna to as South as Patumahoe. Available parameters consist of air quality index (AQI), 10 pollutant levels and four other meteorological variables as per Table \ref{tab:raw-dataset}, with various starting dates (since as early as 2003 in Takapuna) until April 2021.

Only six of the standard-listed air pollutants are monitored and available in the data, and each station independently monitors a subset of the six pollutants. As such, the calculation of AQI, based on available data, may be simplified to

\begin{equation}\label{eq:aqicon} AQI = 100 \times \textrm{max}\{\frac{PM_{2.5}}{25}, \frac{PM_{10}}{50}, \frac{NO_{2}}{200}, \frac{SO_{2}}{350}, \frac{CO}{10}, \frac{O_{3}}{150}\} \end{equation}

\begin{table}[ht]
\begin{center}
\begin{tabular}{lll}
\toprule
Parameter & Unit & Note \\
\midrule
AQI &  & Air quality index \\
BC(370) & ngm\textsuperscript{-3} & Black carbon at 370nm wavelength \\
BC(880) & ngm\textsuperscript{-3} & Black carbon at 880nm wavelength \\
CO\textsuperscript{*} & mgm\textsuperscript{-3} & Carbon monoxide concentration \\
NO & \textmu gm\textsuperscript{-3} & Nitrogen monoxide concentration \\
NO\textsubscript{2}\textsuperscript{*} & \textmu gm\textsuperscript{-3} & Nitrogen dioxide concentration \\
NOx & \textmu gm\textsuperscript{-3} & Nitrogen oxides concentration \\
O\textsubscript{3}\textsuperscript{*} & \textmu gm\textsuperscript{-3} & Ozone concentration \\
PM2.5\textsuperscript{*} & \textmu gm\textsuperscript{-3} & Particulate matter with diameter <2.5\textmu m \\
PM10\textsuperscript{*} & \textmu gm\textsuperscript{-3} & Particulate matter with diameter <10\textmu m \\
SO\textsubscript{2}\textsuperscript{*} & \textmu gm\textsuperscript{-3} & Sulphur dioxide concentration \\
Relative Humidity & \% &  \\
Temperature & $^{\circ}$C &  \\
Wind Speed & ms\textsuperscript{-1} &  \\
Wind Direction & $^{\circ}$ &  \\
\bottomrule
\end{tabular}
\caption{Parameters available in raw data.\\\textsuperscript{*} AQI-related ambient air pollutants}
\label{tab:raw-dataset}
\end{center}
\end{table}

It is noteworthy that the availability of air quality parameters in each monitoring station varies from year to year. Besides, the extreme values addressed in Section \ref{sec:data-extreme-value} are more frequent in earlier years. The final data set is subsetted from the year 2016.

\hypertarget{data-quality-and-cleaning}{%
\section{Data quality and cleaning}\label{data-quality-and-cleaning}}

The raw data consists of two separate data sets, each with a different data structure. Cleaning and manipulation are needed to ensure that the two data sets are consistent in structure and free from error. The raw data sets are individually inspected and cleaned before combination. This section outlines the issues found and methods to address them.

\hypertarget{sec:data-extreme-value}{%
\subsection{Abnormal and missing values}\label{sec:data-extreme-value}}

Abnormal or missing values arise from instrumental or input errors. Upon inspection, 104,332 records were found to have a negative value. Nevertheless, all pollutants are reported in units in the form of mass per unit volume, and other parameters, except for temperature, are only sensible if positive as of Table \ref{tab:raw-dataset}. Therefore, 104,257 records of insensible negative values are removed. Besides, conspicuously anomalous records of AQI are found in data, including consecutive hours of \textgreater1,000 AQI in Takapuna and numerous AQI values being inconsistent with Formula \ref{eq:aqicon} based on available pollutants in the same data set. The anomalous records are nonetheless kept as-is for further verification.

In addition, preliminary inspection finds that 0.81\% of records are explicitly missing. Yet after filling the implicit time gaps in the data, 53.71\% of records are implied to be missing.

\hypertarget{date-and-time}{%
\subsection{Date and time}\label{date-and-time}}

A consistent format in date and time is crucial to the accuracy of temporal data. Observations with inconsistent time format are present in the data, where some are recorded in \texttt{hh:mm:ss} whilst others in \texttt{hh:mm}. The inconsistency in the time format is correctable due to the hourly nature of the data. 0.06\% of records with missing time are removed.

The time zone of New Zealand changes by +1 during daylight saving. To avoid duplicated index upon boundaries of daylight saving upon data visualisation, all time-stamps are presented in NZST (UTC+12). On the other hand, the date and time in the cleaned data file are stored as a single variable, with its format in compliance with ISO 8601 \autocite{iso8601,readr}.

\hypertarget{duplicate-records}{%
\subsection{Duplicate records}\label{duplicate-records}}

Temporal data should not present duplicate records. Of the 7,292,038 valid records, 239,374 (3.28\%) are duplicate with 120,207 redundant records. Further checking reveals that 230,822 of the duplicates have inconsistent values. However, as the scale of the inconsistency of most duplicate records is reasonably small, the first-appearing records of each duplicate are kept.

\hypertarget{structural-difference-in-raw-data-sets}{%
\subsection{Structural difference in raw data sets}\label{structural-difference-in-raw-data-sets}}

The primary data set, which records all parameters except for wind direction, is in long format, with each observation consisting of a single record of one parameter for one station at a given hour. Nevertheless, each observation of the wind direction data set consists of wind direction records of all stations at a given hour. Each data set is pivoted to the structure such that each observation is uniquely identified by the date-time and station with records of all parameters before combination to ensure structural consistency.

\hypertarget{data-enrichment}{%
\section{Data enrichment}\label{data-enrichment}}

\begin{itemize}
\tightlist
\item
  Categorisation of AQI
\item
  Categorisation of wind direction
\end{itemize}

\hypertarget{ch:design}{%
\chapter{Design layout and philosophy}\label{ch:design}}

\begin{itemize}
\tightlist
\item
  Overview of AQI

  \begin{itemize}
  \tightlist
  \item
    Spatial
  \item
    Temporal (AQI and its constituent)

    \begin{itemize}
    \tightlist
    \item
      Calendar
    \item
      Drill-down line plot
    \end{itemize}
  \end{itemize}
\item
  Data enrichment

  \begin{itemize}
  \tightlist
  \item
    Explore relationship AQI with wind speed and direction
  \item
    Meteorological data
  \end{itemize}
\item
  Trend analysis
\end{itemize}

\hypertarget{ch:linking}{%
\chapter{Linked interactive graphics}\label{ch:linking}}

\begin{itemize}
\tightlist
\item
  Introduction
\item
  Implementation of interactive linking
\item
  Modularisation of Shiny App
\end{itemize}

\hypertarget{ch:model}{%
\chapter{Modelling}\label{ch:model}}

\hypertarget{ch:conclusion}{%
\chapter{Conclusion and future works}\label{ch:conclusion}}

\appendix

\hypertarget{additional-stuff}{%
\chapter{Additional stuff}\label{additional-stuff}}

You might put some computer output here, or maybe additional tables.

Note that line 5 must appear before your first appendix. But other appendices can just start like any other chapter.

\printbibliography[heading=bibintoc]



\end{document}
